{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "-------\n",
    "\n",
    "## Overview (近朱者赤，近墨者黑。)\n",
    "K-nearest Neighbor algorithm (k-NN) is a non-parametric method used for classification and regression.\n",
    "\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. \n",
    "\n",
    "It only based one or several nearest neighbors to determine its category or value. And since it only rely on limited number of neighbors, KNN is more preferable when the data overlap a lot.\n",
    "![](media/15907186358436/15907194200984.jpg)\n",
    "\n",
    "-------\n",
    "\n",
    "## Basic Concept\n",
    "We measure the distance from **query point** to each sample points, find the **K-nearest neighbors**, and determine the **label** or **value** of this query point.\n",
    "### 1. Distance Metrics\n",
    "We use **distance** to measure the similarity of different samples. Euclidean distance, Minkowski Distance, Mahalanobis Distance, Haversine Distance and Manhattan Distance are most commonly used.\n",
    "\n",
    "* Euclidean distance\n",
    "    * Euclidean distance\n",
    "$$\n",
    "\\begin{align*}\n",
    "& Distance\\ from\\ x_1\\ to\\ x_2:\\\\\\\\\n",
    "& d(x_1,x_2) = \\sqrt{\\sum_{i=1}^{N}(x_{1i}-x_{2i})^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "    But normally scales in different features are different, or in other words different scale in different dimension, we prefer to standardize the distance.\n",
    "    \n",
    "    * Standardized Euclidean Distance\n",
    "$$\n",
    "\\begin{align*}\n",
    "& Distance\\ from\\ x_1\\ to\\ x_2:\\\\\\\\\n",
    "& x_{I}^{'} = \\frac{x_i-\\mu_i}{s_i}\\\\\\\\\n",
    "& \\mu: mean,\\ s:standard\\ variance\\\\\\\\\n",
    "& d(x_1,x_2) = \\sqrt{\\sum_{i=1}^{N}(\\frac{x_{1i}-x_{2i}}{s_i})^2}\n",
    "\\end{align*}\n",
    "$$    \n",
    "    \n",
    "* Manhattan Distance\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    d(x_1,x_2) = \\sum ( \\left |x_{1i} - x_{2i} \\right |)\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    \n",
    "* Minkowski Distance\n",
    "    \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    & d(x_1,x_2) = \\Big(\\sum_{i=1}^{N} \\left | x_{1i} - x_{2i}\\right |^p\\Big )^{(1/p)}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    \n",
    "    * When p = 1, it is Manhattan Distance;\n",
    "    * When p = 2, it is Euclidean Distance;\n",
    "    * When p is infinite, it is Chebyshev Distance.\n",
    "    \n",
    "    **Pitfalls:**\n",
    "    1. Scale matters, if scales in different dimensions are different, these distance can not be applied unless standardizing them.\n",
    "    2. Distribution in each dimension might be different.\n",
    "    3. We assume that features are independent to each other.\n",
    "    \n",
    "* Mahalanobis Distance\n",
    "\n",
    "    Since the disadvantage of Euclidean distance, we introduce Mahalanobis Distance. This distance utilizes covariance matrix $\\Sigma$ to offset the impact of different scales. \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    & d(X,Y) = \\sqrt{(X-Y)^T\\Sigma^{-1}(X-Y)}\n",
    "    \\end{align*}\n",
    "    $$   \n",
    "    \n",
    "* Haversine Distance\n",
    "\n",
    "### 2. Choosing K\n",
    "#### a. Impact of K\n",
    "\n",
    "* Large K\n",
    "If K is too large, then we will take those not similar points into consideration. The system bias will be low but the variance will be high. The system will be very robust.\n",
    "* Small K\n",
    "If K is too small, the result may easily be affected by noise. The bias will be high and variance will be low. The system will be very sensitive. \n",
    "\n",
    "#### b. How to choose the best K\n",
    "\n",
    "* **Using Cross Validation**\n",
    "\n",
    "* Empirical rule: K is normally less than the square root of number of sample size.\n",
    "\n",
    "### 3. Decision Rule\n",
    "\n",
    "* **Classification:**  The majority wins.\n",
    "* **Regression:** Mean of K-nearest neighbors.\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier():\n",
    "    \"\"\"\n",
    "    KNN Classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ------------------\n",
    "    K: int\n",
    "        K nearest neighbors. default 1 to m(Sample Size)\n",
    "    distance: string\n",
    "        distance metrics\n",
    "    weight: (Not Using)\n",
    "        \n",
    "    Attributes\n",
    "    ------------------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,algorithm = 'brute', distance = 'Euclidean',K=1,p=1):\n",
    "\n",
    "        self.K = K\n",
    "        \n",
    "        self.algorithm = algorithm\n",
    "        \"\"\"\n",
    "        3 algorithms\n",
    "        \"\"\"\n",
    "        \n",
    "        self.distance = distance\n",
    "        \n",
    "        # When using _Minkowski_Distance, p needs to be set\n",
    "        self.p = p\n",
    "        \n",
    "        if self.distance == 'Euclidean':\n",
    "            self.distance_metrics = self._Euclidean_Distance\n",
    "        elif self.distance == 'Manhattan':\n",
    "            self.distance_metrics = self._Manhattan_Distance\n",
    "        elif self.distance == 'Minkowski':\n",
    "            self.distance_metrics = self._Minkowski_Distance\n",
    "        elif self.distance == 'Mahalanobis':\n",
    "            self.distance_metrics = self._Mahalanobis_Distance\n",
    "        elif self.distance == 'Haversine':\n",
    "            self.distance_metrics = self._Haversine_Distance\n",
    "    \n",
    "    def _Euclidean_Distance(self, X, Dataset):\n",
    "        m,n = Dataset.shape\n",
    "        diff = np.tile(X,(m,1)) - Dataset\n",
    "        sqDiff = diff**2\n",
    "        sumSqDiff = sqDiff.sum(axis=1)\n",
    "        dist = sumSqDiff ** 0.5\n",
    "        return dist\n",
    "    \n",
    "    def _Manhattan_Distance(self, X, Dataset):\n",
    "        m,n = Dataset.shape\n",
    "        diff = np.tile(X,(m,1)) - Dataset\n",
    "        absDiff = np.abs(diff)\n",
    "        dist = absDiff.sum(axis=1)\n",
    "        return dist\n",
    "    \n",
    "    def _Minkowski_Distance(self, X, Dataset):\n",
    "        m,n = Dataset.shape\n",
    "        diff = np.tile(X,(m,1)) - Dataset\n",
    "        absDiff = np.abs(diff)\n",
    "        sqAbsDiff = absDiff**self.p\n",
    "        totalDiff = sqAbsDiff.sum(axis=1)\n",
    "        dist = totalDiff**(1/self.p)\n",
    "        return dist\n",
    "    \n",
    "    def _Mahalanobis_Distance(self, X, Dataset):\n",
    "        pass\n",
    "    \n",
    "    def _Haversine_Distance(self, X, Dataset):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def fit(self,X_train,y_train,verbose= True):\n",
    "        \"\"\"\n",
    "        Fit method for training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------------------\n",
    "        X_train: {array-like}, shape = [n_samples, n_features]\n",
    "            Training matrix, where 'n_samples' is the number of samples \n",
    "            and 'n_features' is the number of features\n",
    "        y_train: {array-like}, shape = [n_samples]\n",
    "            Target labels\n",
    "        \n",
    "        Attributes:\n",
    "        -----------------------\n",
    "        d_record_: list\n",
    "            Record all distance.\n",
    "        error_rate_: list\n",
    "            Record all missclassification rate.  \n",
    "        \n",
    "        Returns:\n",
    "        ------------------------\n",
    "        self: object\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.algorithm == 'brute':\n",
    "            self.Dataset = X_train\n",
    "            self.Label = y_train\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        m,n = X_test.shape\n",
    "        prediction = []\n",
    "        for i in range(m):\n",
    "            \n",
    "            Dist = self.distance_metrics(X_test[i,:],self.Dataset)\n",
    "            SortedDistIndex = np.argsort(Dist)\n",
    "            \n",
    "            classCount = defaultdict(int)\n",
    "            for j in range(self.K):\n",
    "                lab = self.Label[SortedDistIndex[j]][0]\n",
    "                classCount[lab] += 1\n",
    "            sortedPredict = sorted(classCount.items(), key=lambda item: item[1])\n",
    "            prediction.append(sortedPredict[0][0])\n",
    "        \n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "feature = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "target = pd.DataFrame(iris.target,columns=['target'])\n",
    "df = pd.concat([feature,target],axis=1)\n",
    "df = df[df['target']!=2]\n",
    "df =df.reset_index()\n",
    "target = df['target'].to_frame()\n",
    "feature = df.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature.to_numpy()\n",
    "#data_mat = np.asmatrix(feature)\n",
    "y_train = target.to_numpy()\n",
    "#label_mat = np.asmatrix(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier(distance = 'Minkowski', p = 1, K = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0],\n",
       "       [ 0, 50]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
