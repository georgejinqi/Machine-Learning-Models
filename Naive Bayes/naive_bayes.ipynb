{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gaussian Naive Bayes Classifier\n",
    "\n",
    "$$\n",
    "P(x_i|y)=\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}\\exp \\Big(-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2}\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    \n",
    "    def __init__(self, priors = None, minimum_var = 1e-09):\n",
    "        \"\"\"\n",
    "        GaussianNB Classifier.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        priors: {array-like} \n",
    "        \n",
    "            set priors for each class if it is known, unless set it as None\n",
    "        \n",
    "        minimum_var: float\n",
    "            \n",
    "            To avoid 0 variance.\n",
    "\n",
    "        Attributes\n",
    "        ------------------        \n",
    "        \"\"\"\n",
    "        self._class_prior = priors\n",
    "        self._class_count = None\n",
    "        self._classes = None\n",
    "        self._var =None\n",
    "        self._mean = None\n",
    "        self.minimum_var = minimum_var\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Training model\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        X_train : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y_train : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check datatype \n",
    "        if isinstance(X_train,np.ndarray) and isinstance(y_train,np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "            except:\n",
    "                raise TypeError('numpy.array required for input data')\n",
    "        \n",
    "        # Sample and feature size\n",
    "        \n",
    "        m_sample, n_feature = X_train.shape \n",
    "        \n",
    "        # Find all unique classes\n",
    "        self._classes = np.unique(y_train)\n",
    "        self._class_count = len(self._classes)\n",
    "        \n",
    "        # Initialize man, var, prior for each class\n",
    "        if self._class_prior == None: # No priors\n",
    "            \n",
    "            self._class_prior = np.zeros(self._class_count,dtype=np.float64)\n",
    "            self._var = np.zeros((self._class_count,n_feature),dtype=np.float64)\n",
    "            self._mean = np.zeros((self._class_count,n_feature),dtype=np.float64)\n",
    "\n",
    "            for idx, cl in enumerate(self._classes):\n",
    "                X_cl = X_train[y_train==cl]\n",
    "                self._class_prior[idx] = X_cl.shape[0]/float(m_sample)\n",
    "                self._mean[idx, :] = X_cl.mean(axis=0) # along column\n",
    "                self._var[idx, :] = X_cl.var(axis=0) \n",
    "\n",
    "        else:\n",
    "            \n",
    "            self._var = np.zeros((self._class_count,n_feature),dtype=float)\n",
    "            self._mean = np.zeros((self._class_count,n_feature),dtype=float)\n",
    "\n",
    "            for idx, cl in enumerate(self._classes):\n",
    "                X_cl = X_train[y_train==cl]\n",
    "                self._mean[idx, :] = X_cl.mean(axis=0) # along column\n",
    "                self._var[idx, :] = X_cl.var(axis=0) \n",
    "        \n",
    "        self._var[:,:] += self.minimum_var\n",
    "                \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        y_pred = [self._predict(x_test) for x_test in X_test]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    def _predict(self, x_test):\n",
    "        \n",
    "        posteriors = []\n",
    "        \n",
    "        # Instead of multiply all probabilities, we calculate the sum of log(p)\n",
    "        \n",
    "        for idx, cl in enumerate(self._classes):\n",
    "            \n",
    "            likelihood = np.sum(np.log(self._pdf(idx,x_test)))\n",
    "            prior = np.log(self._class_prior[idx])\n",
    "            \n",
    "            posterior = likelihood + prior\n",
    "            \n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        # return the class with highest posterior\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "        \n",
    "    \n",
    "    def _pdf(self, class_idx, x_test):\n",
    "        \n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(- (x_test - mean)**2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator/denominator\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Multinomial Naive Bayes\n",
    "\n",
    "Likelihood\n",
    "\n",
    "$$\n",
    "P(x_i|y)=\\frac{Count(x_i|y)+\\alpha}{Count(y)+\\alpha|V|}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "P(x_i|y)=\\frac{\\sum_{i=1}^{N}I(x_i,y)+\\alpha}{\\sum_{i=1}^{N}I(y)+\\alpha|V|}\n",
    "$$\n",
    "where $|V|$ is the number of different categories in $x_i$, and $\\alpha$ is a smoothing parameter and $\\alpha>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to [Kenzo takahashi](https://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultinomialNB:\n",
    "    \n",
    "    def __init__(self, alpha = 1.0):\n",
    "        \"\"\"\n",
    "        MultinomialNB Classifier.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        priors: {array-like} \n",
    "        \n",
    "            set priors for each class if it is known, unless set it as None\n",
    "            \n",
    "        alpha: float, default = 1.0 \n",
    "        \n",
    "            alpha = 1.0: Laplace smoothing\n",
    "            alpha in (0,1): Lidstone\n",
    "            alpha = 0: No smoothing\n",
    "        \n",
    "\n",
    "        Attributes\n",
    "        ------------------ \n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self._classes = None\n",
    "        self._class_count = None\n",
    "        self._classes_groups = None\n",
    "        self._class_log_prior = None\n",
    "        self._feature_count = None\n",
    "        self._feature_log_prob = None\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        \"\"\"\n",
    "        Training model\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        X_train : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y_train : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check datatype \n",
    "        if isinstance(X_train,np.ndarray) and isinstance(y_train,np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "            except:\n",
    "                raise TypeError('numpy.array required for input data')\n",
    "        \n",
    "        # Sample and feature size\n",
    "        \n",
    "        m_sample, n_feature = X_train.shape  \n",
    "        \n",
    "        # Find all unique classes\n",
    "        self._classes = np.unique(y_train)\n",
    "        # Group samples by its class \n",
    "        self._classes_groups = [[x for x, t in zip(X_train, y_train) if t == c] for c in np.unique(y_train)]\n",
    "        # Count numner of samples in each class\n",
    "        self._class_count = [len(x) for x in self._classes_groups] \n",
    "        # Calculate the log prior of each class\n",
    "        self._class_log_prior =  [np.log(len(cl)/float(m_sample)) for cl in self._classes_groups]\n",
    "        # Count each feature/word for each class\n",
    "        self._feature_count = np.array([np.array(cl).sum(axis=0) for cl in self._classes_groups], dtype=float) \n",
    "        # Add alpha to self._feature_count\n",
    "        count = self._feature_count + self.alpha\n",
    "        # Caculate log conditional probability \n",
    "        # [np.newaxis]: np can not transpose 1-D array only by .T\n",
    "        self._feature_log_prob = np.log( count / count.sum(axis=1)[np.newaxis].T)\n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        # Check datatype \n",
    "        if isinstance(X_test,np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X_test = np.array(X_test)\n",
    "            except:\n",
    "                raise TypeError('numpy.array required for input data')\n",
    "        \n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return predictions\n",
    "\n",
    "                \n",
    "    def _predict(self, x):\n",
    "\n",
    "        posteriors = [(self._feature_log_prob*x).sum(axis=1) + self._class_log_prior]\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BernoulliNB\n",
    "\n",
    "Similar to multinomial naive bayes, but only include binary values, number or boolean.\n",
    "\n",
    "$$\n",
    "P(x_i|y)=P(i|y)x_i + (1-P(i|y)(1-x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BernoulliNB:\n",
    "    \n",
    "    def __init__(self, alpha = 1.0, binarize = None):\n",
    "        \"\"\"\n",
    "        BernoulliNB Classifier.\n",
    "\n",
    "        Parameters\n",
    "        ------------------\n",
    "        priors: {array-like} \n",
    "        \n",
    "            set priors for each class if it is known, unless set it as None\n",
    "            \n",
    "        alpha: float, default = 1.0 \n",
    "        \n",
    "            alpha = 1.0: Laplace smoothing\n",
    "            alpha in (0,1): Lidstone\n",
    "            alpha = 0: No smoothing\n",
    "        \n",
    "        binarize: float, default = None\n",
    "        \n",
    "            The threshold for binarizing of sample feature. If None, \n",
    "            input is presumed to already consist of binary vectors.\n",
    "\n",
    "        Attributes\n",
    "        ------------------ \n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self._classes = None\n",
    "        self._class_count = None\n",
    "        self._classes_groups = None\n",
    "        self._class_log_prior = None\n",
    "        self._feature_count = None\n",
    "        self._feature_prob = None\n",
    "        self.binarize = binarize\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        \"\"\"\n",
    "        Training model\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        X_train : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y_train : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check datatype \n",
    "        if isinstance(X_train,np.ndarray) and isinstance(y_train,np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "            except:\n",
    "                raise TypeError('numpy.array required for input data')\n",
    "                \n",
    "        # Check if X_train is binary\n",
    "        if self.binarize == None:\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                np.where(X_train>self.binarize, 1, 0)\n",
    "            except:\n",
    "                raise TypeError('X requires to be binary')\n",
    "\n",
    "        \n",
    "        # Sample and feature size\n",
    "        m_sample, n_feature = X_train.shape  \n",
    "        \n",
    "        # Find all unique classes\n",
    "        self._classes = np.unique(y_train)\n",
    "        # Group samples by its class \n",
    "        self._classes_groups = [[x for x, t in zip(X_train, y_train) if t == c] for c in np.unique(y_train)]\n",
    "        # Count numner of samples in each class\n",
    "        self._class_count = [len(x) for x in self._classes_groups]\n",
    "        # Calculate the log prior of each class\n",
    "        self._class_log_prior =  [np.log(len(cl)/float(m_sample)) for cl in self._classes_groups]\n",
    "        # Count each feature/word for each class\n",
    "        self._feature_count = np.array([np.array(cl).sum(axis=0) for cl in self._classes_groups], dtype=float) \n",
    "        # Caculate conditional log probability \n",
    "        # [np.newaxis]: np can not transpose 1-D array only by .T\n",
    "        self._feature_log_prob = np.log( (self._feature_count + self.alpha)  / (np.array(self._class_count) + 2*self.alpha)[np.newaxis].T)\n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        # Check datatype \n",
    "        if isinstance(X_test,np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                X_test = np.array(X_test)\n",
    "            except:\n",
    "                raise TypeError('numpy.array required for input data')\n",
    "        \n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return predictions\n",
    "\n",
    "                \n",
    "    def _predict(self, x):\n",
    "\n",
    "        posteriors = [(self._feature_log_prob*x - self._feature_log_prob*(1-x)).sum(axis=1) + self._class_log_prior]\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model accuracy 0.965 vs sklearn Model 0.965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from naivebayes import NaiveBayes\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "gnb = GNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "predictions2 = gnb.predict(X_test)\n",
    "\n",
    "print(\"Current Model accuracy {} vs sklearn Model {}\".format (accuracy(y_test, predictions),accuracy(y_test, predictions2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
