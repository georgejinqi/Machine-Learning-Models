# Decision Tree
## Overview
A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.

A decision tree utilizes all data from dataset, and each data finally locates on leaf-nodes.

It can be used for ***regression*** and ***classification***.

There are 3 steps for building a decision tree:
1. Feature selection
2. Generate decision tree: **Mutual Exclusivity** and **Completeness**
3. Prune

## Tree Structure 
Decision tree is generated by ***nodes***, ***directed edges***.
Nodes include:
* **Internal node**: features
* **Leaf node**: labels

## Feature Selection
#### 1. Information
Information can be viewed as the ***degree of surprise***. If one thing is for sure, for example tomorrow Trump is still an asshole, then this gives us no information because we know it always holds. If one thing is unlikely to happen, for example Trump stop Twittering, then everyone will be surprised, and this message gives us a huge amount of information.

So it is easy to come up with the relationship between information and probability:

1. **Information $h(\cdot)$ is monotonically decreasing in probability $p(\cdot)$:** an increase in the probability of an event decreases the information from an observed event, and vice versa.
2. **$h(\cdot)\ge 0$:** information can not be negative.
3. **$h(1)= 0$:** events that always occur do not communicate information.
4. **$h(x,y) = h(x)+h(y)$:** when event x and event y are exactly independent, information is additive. ***$p(x,y)=p(x)\cdotp(y)$***

Based the last very important property, we can find a proper relationship between information and probability:

$$h(x) = -log\ p(x)$$

Since p(x) is between 0 and 1, the negative sign makes sure that h(x) is non-negative.

#### 2. Entropy
If there is a distribution where event $i$ can happen with probability $p_i$, and it is sampled $N$ times with an outcome $i$ occurring $n_i = N\times p_i$ times, the total amount of information we have received is:

$$
\sum_{i} n_i h(x_i) =- \sum_{i} N p(x_i)\ logp(x_i)
$$

Then the average of total amount of information is ***Entropy***:

$$
H(x) = - \sum_{i}^{N} p(x_i)\ logp(x_i)
$$

#### 3. Information Gain (ID3)
When introducing a feature X, then entropy (Uncertainty) of Y will decrease. The amount of decrease is ***information we gained*** from X, and we prefer a X that leads to a large amount of information gain.

For example, the probability of rainy weather (Y) tomorrow is 0.5, so the entropy of it is 1. And now it starts lightening (X), the probability of Y increase to 0.9, entropy decrease to 0.15. Because of introducing X (lightening), now the information we get from Y given X decreases $(1 - 0.15) = 0.85$, the amount of information we gain is $0.85$. ***Love it***

$$
IG(Y|X) = H(Y) - H(Y|X)
$$

**Note:** Suppose there exists a X, for example ***Index***, even though it is meaningless but it leads to correctly classify all Y, then $H(Y|X) = 0$, and ***Information Gain*** is ***maximum***. ID3 algorithm prefer choosing features like this, which will lead to ***over-fitting***.

#### 4. Information Gain Ratio (C4.5)
In order to deal the problem we mentioned in ID3, we add a ***Penalty*** to X that has too many categories. If a X has too many categories, then $H_X(Y)$ will be huge, which offsets its high information gain.


$$
\begin{align*}
& IGR(Y|X)=\frac{IG(Y|X)}{H_X(Y)}
\\\\
& where \ H_X(Y) = -\sum_{i=1}^{v} \frac{|Y_i|}{|Y|} log \frac{|Y_i|}{|Y|}\\\\
& Note:\\\\
& 1.\ v\ is\ the \ number \ of \ different\ categories\ in\ feature\ X\\\\
& 2.\ |Y_i|\ is \ the\ number\ of \ samples\ corresponding\ to \ ith\ category \ in \ X\\\\
& 3.\ |Y| \ is \ the \ total\ number\ of\ samples
\end{align*}
$$

**Note:** Logarithm requires huge amount of computation power.

#### 5. Gini Index (CART)
Since Logarithm requires heavily computation, we want to find another way to calculate the uncertainty or impurity of information.
* **Gini Impurity**

    Expand $log(x)$ when $x=1$ by using ***1st Taylor Polynomial***.
    
    $$
    \begin{align*}
    H(X) & =-\sum_{x\in X} p(x)logp(x)\\\\
    log(x) & \simeq log(x_0) + log'(x_0)(x-x_0)\\\\
    & =log(1) + log'(1)(x-1)\\\\
    & =x-1\\\\
    H(X) & \simeq -\sum_{x \in X} p(x)[p(x)-1]\\\\
    & = \sum_{x \in X}p(x)[1-p(x)]\\\\
    & = G(X)\\\\
    G(X) & = \sum_{x \in X}p(x)[1-p(x)]\\\\
    & = \sum_{x \in X}p(x) - p^2(x)\\\\
    & = 1 - \sum_{x \in X}p^2(x)
    \end{align*}
    $$
* **Total Gini Impurity $C(T)$**

    Equals to ***Weighted Average of Gini Impurities by the leaf nodes***
    
    Each leaf node has different weight on Gini impurity.
    
    
    $$
    \begin{align*}
    & C(T) = \sum_{t \in leaves} w_tG(t)\\\\
    & where \ w_t = \frac{n(t)}{N},N\ is\ the\ total\ number\ of\ samples\\
    \end{align*}
    $$
    
        Example:

![Example](https://github.com/uttgeorge/Machine-Learning-Models/blob/master/Decision%20Tree/media/Decision%20Tree%20Diagram.png)

$$
\begin{align*}
G_1(x) & = 1 - (\frac{N_1}{N_1+N_2})^2 - (\frac{N2}{N1+N2})^2\\\\
G_2(x) & = 1 - (\frac{N_3}{N_3+N_4})^2 - (\frac{N4}{N3+N4})^2\\\\
Total\ Gini\ Impurity: G(x) & = \sum_{i=1}^{2}w_iG_i(x)\\\\
w_i: & \Bigg \lbrace \begin{matrix}w_1 & = \frac{N_1+N_2}{N_1+N_2+N_3+N_4}\\\\
w_2 & =\frac{N_3+N_4}{N_1+N_2+N_3+N_4}\end{matrix}
\end{align*}
$$



#### 6. **Feature Selection:** 

Choose the ***Lowest*** $C(T)$, which means the data are more pure.
    
    
## Generate Decision Tree (CART)
#### A. Classification Tree

Start from the root, train the binary tree.

1. Dataset correspond to current node: $D = {training data}$.
    a. Continuous Features
    * Sort: Low --> High
    * Calculate the **average** of ***all adjacent data point***: $a_i$
    * Split $D$ into $D_1$ and $D_2$ by $a_i$

    b. Discrete Features
    * Ranked Data (eg. 1, 2, 3, 4)
        * Use every value ***except*** the last one (eg. 4) to split D
    * Categorical Data (eg. Red, Green, Blue)
        * Use every combination ***except*** the **universal set** (eg. $[R, G, B]$):
        $[R],[G],[B],[R,G],[R,B],[G,B]$
        
2. Calculate Gini Scores for all features X with their split points S. Choose the feature and its split point S with **the minimum Gini score** as the current node, and split dataset $D$ at this node into $D_1$ and $D_2$.
3. Repeat step1 and step2 on child nodes, till meet the terminative condition.

    **Note:** Step3 will stop automatically when the Gini Score of child nodes is higher than their parent's Gini Score. ***(Auto Feature Selection)***
    
#### B. Regression Tree

***Objective: Minimize sum of square error.***

Start from the root, train the binary tree.

1. Dataset correspond to current node: $D = {training data}$.
    
    a. Continuous Features
    * Sort: Low --> High
    * Calculate the **average** of ***all adjacent data point***: $a_i$
    * Split $D$ into $D_1$ and $D_2$ by $a_i$
    * Calculate the average $y_1$,$y_2$ (Target) for $D_1$ and $D_2$.

    b. Discrete Features
    * Ranked Data (eg. 1, 2, 3, 4)
        * __Same as Classification Tree__
    * Categorical Data (eg. Red, Green, Blue)
        * Use every combination ***except*** the **universal set** (eg. $[R, G, B]$):
        $[R],[G],[B],[R,G],[R,B],[G,B]$
        * Calculate the average $y_1$,$y_2$ (Target) for $D_1$ and $D_2$.
        
2. For all features **J** and their corresponding split points **S**, calculate **Least Square Error** of $y_1$ and $y_2$. Choose the **x** and **s** that has the minimum error, use that **s** split **D** into $D_1$ and $D_2$, and calculate their **averages**.

$$
\begin{align*}
& \underset{J,S}{min} \Big [\underset{C_1}{min} \sum_{X_i \in R_1(j,s)}(y_i-C_1)^2 + \underset{C_2}{min} \sum_{X_i \in R_2(j,s)}(y_i-C_2)^2 \Big ]\\\\
& s.t.\\\\
& R_1(j,s) = \lbrace x|x^{(j)}\le s \rbrace\\\\
& R_2(j,s) = \lbrace x|x^{(j)}> s \rbrace\\\\
& C_m=\frac{1}{N_m}\sum_{X_i \in R_m(j,s)}y_i\\\\
& x \in R_m, m = 1, 2
\end{align*}
$$


3. Repeat step1 and step2 on child nodes, till meet the terminative condition.
    * All records in a leaf belongs to same class.
    * Minimum Sum of Square Error (SSE) is larger than some threshold.

4. Output Values:

    $
    f(x) = \sum_{m=1}^{M}c_mI(x \in R_m)
    $
    
    ***Each leaf $R_m$ has a output value $C_m$***
    
## Pruning: a. Smallest Error  b. Smallest Tree
#### Pre and Post Prune
1. Pre-pruning: early stopping

    a. Set min_depth
    b. Set min_sample
    c. Set min_leaves
    d. Set min_Gini_threshold
    e. Chi-square test of Independent
     
1. Post-pruning
    a. Grow a perfect tree
    b. Prune the nodes buttom-up
    
#### Cost Complexity Function (Weak Link Pruning)

***We want to find a balance between minimum cost and minimum tree.***

$$
\begin{align*}
& min: R_{\alpha}(T)=R(T)+ \alpha |T|\\\\
where:\ & R(T)\ is\ the\ total\ error.\\\\
&  |T|\ is\ the\ number\ of \ terminal\ nodes\ of \ tree\ T.\\\\
& \alpha\ is \ a \ regularization\ parameter.
\end{align*}
$$

**For regression:**


$$
\begin{align*}
R(T) & =\sum_{m=1}^{|T|}\sum_{X_i \in R_m}(y_i - \hat{y}_{R_m})^2\\\\
& s.t.\ T \subset T_0\\\\
where\ & R_m \ is\ mth\ terminal\ node.\\\\
& \hat{y}_{R_m}
\end{align*}
$$



**For classification:**

$$
\begin{align*}
R(T) & =\sum_{t \in |T|}R(t)\\\\
& = \sum_{t \in |T|}w_tG(t)\\\\
& s.t.\ T \subset T_0\\\\
where\ w_t=\frac{n(t)}{N} \ & with\ n(t)\ being\ the\ number\ of\ records\ in \ leaf\ t\\\\
& and\ N \ being\ total \ number\ of\ records.\\\\
\end{align*}
$$

#### Prune a subtree

***Now prune a subtree $T_t$***

$R_{\alpha}(T-T_t)-R_{\alpha}(T)$ is the variation of the cost-complexity when pruning a subtree $T_t$.

Now set:
* T: a tree that is going to be pruned.
* $T_t$: a subtree with root t that is being cut off.
* t: the root of $T_t$.


***Question arises.***

How to choose a subtree $T_t$, such that overall error does not change or has a very small change after pruning it? In other words, how to minimize $R_{\alpha}(T-T_t)-R_{\alpha}(T)$.

***Cut off $T_t$ will leave node t as a leaf. (A terminal node)***

$$
\begin{align*}
Obj:\ min\ & R_{\alpha}(T-T_t)-R_{\alpha}(T)\\\\
= & R(T-T_t) -R(T)+\alpha(|T-T_t|-|T|)\\\\
= & R(t)-R(T_t)+\alpha(1-|T_t|)\\\\
\end{align*}
$$

To explain this equation, let's draw a simple tree.

    

## References
[wikipedia](https://en.wikipedia.org/wiki/Decision_tree)