# Decision Tree
## Overview
A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.

A decision tree utilizes all data from dataset, and each data finally locates on leaf-nodes.

It can be used for **regression** and **classification**.

There are 3 steps for building a decision tree:
1. Feature selection
2. Generate decision tree: **Mutual Exclusivity** and **Completeness**
3. Prune

## Tree Structure 
Decision tree is generated by **nodes**, **directed edge**.
Nodes include:
* **Internal node**: features
* **Leaf node**: labels

## Feature Selection
#### 1. Information
Information can be viewed as the **degree of surprise**. If one thing is for sure to happen, for example tomorrow Trump is still an asshole, then this gives us no information because we know it always holds. If one thing is unlikely to happen, for example Trump stop Twittering, then everyone will be surprised, and this message gives us a huge amount of information.

So it is easy to come up with the relationship between information and probability:

1. **Information $h(\cdot)$ is monotonically decreasing in probability $p(\cdot)$:** an increase in the probability of an event decreases the information from an observed event, and vice versa.
* **$h(\cdot)\ge 0$:** information can not be negative.
* **$h(1)= 0$:** events that always occur do not communicate information.
* **$h(x,y) = h(x)+h(y)$:** when event x and event y are exactly independent, information is additive. **$p(x,y)=p(x)\cdotp(y)$**

Based the last very important property, we can find a proper relationship between information and probability:

$$h(x) = -log\ p(x)$$

Since p(x) is between 0 and 1, the negative sign makes sure that h(x) is non-negative.

#### 2. Entropy
If there is a distribution where event $i$ can happen with probability $p_i$, and it is sampled $N$ times with an outcome $i$ occurring $n_i = N\times p_i$ times, the total amount of information we have received is:

$$
\sum_{i} n_i h(x_i) =- \sum_{i} N p(x_i)\ logp(x_i)
$$

Then the average of total amount of information is **Entropy**:

$$
H(x) = - \sum_{i}^{N} p(x_i)\ logp(x_i)
$$

#### 3. Information Gain (ID3)
When introducing a feature X, then entropy (Uncertainty) of Y will decrease. The amount of decrease is **information we gained** from X, and we prefer a X that leads to a large amount of information gain.

For example, the probability of rainy weather (Y) tomorrow is 0.5, so the entropy of it is 1. And now it starts lightening (X), the probability of Y increase to 0.9, entropy decrease to 0.15. Because of introducing X (lightening), now the information we get from Y given X decreases $(1 - 0.15) = 0.85$, the amount of information we gain is $0.85$. **Love it**

$$
IG(Y|X) = H(Y) - H(Y|X)
$$

**Note:** Suppose there exists a X, for example **Index**, even though it is meaningless but it leads to correctly classify all Y, then $H(Y|X) = 0$, and **Information Gain** is **maximum**. ID3 algorithm prefer choosing features like this, which will lead to **over-fitting**.

#### 4. Information Gain Ratio (C4.5)
In order to deal the problem we mentioned in ID3, we add a **Penalty** to X that has too many categories. If a X has too many categories, then $H_X(Y)$ will be huge, which offsets its high information gain.


$$
\begin{align*}
& IGR(Y|X)=\frac{IG(Y|X)}{H_X(Y)}
\\\\
& where \ H_X(Y) = -\sum_{i=1}^{v} \frac{|Y_i|}{|Y|} log \frac{|Y_i|}{|Y|}\\\\
& Note:\\ \\ 
& 1.\ v\ is\ the \ number \ of \ different\ categories\ in\ feature\ X\\\\
& 2.\ |Y_i|\ is \ the\ number\ of \ samples\ corresponding\ to \ ith\ category \ in \ X\\\\
& 3.\ |Y| \ is \ the \ total\ number\ of\ samples
\end{align*}
$$

**Note:** Logarithm requires huge amount of computation power.

#### 5. Gini Index (CART)
Since Logarithm requires heavily computation, we want to find another way to calculate the uncertainty or impurity of information.
* **Gini Impurity**

    Expand $log(x)$ when $x=1$ by using **1st Taylor Polynomial**.
    
    $$
    \begin{align*}
    H(X) & =-\sum_{x\in X} p(x)logp(x)\\\\
    log(x) & \simeq log(x_0) + log'(x_0)(x-x_0)\\\\
    & =log(1) + log'(1)(x-1)\\\\
    & =x-1\\\\
    H(X) & \simeq -\sum_{x \in X} p(x)[p(x)-1]\\\\
    & = \sum_{x \in X}p(x)[1-p(x)]\\\\
    & = G(X)\\\\
    G(X) & = \sum_{x \in X}p(x)[1-p(x)]\\\\
    & = \sum_{x \in X}p(x) - p^2(x)\\\\
    & = 1 - \sum_{x \in X}p^2(x)
    \end{align*}
    $$
* **Total Gini Impurity $C(T)$**

    Equals to **Weighted Average of Gini Impurities by the leaf nodes**
    
    $$
    \begin{align*}
    & C(T) = \sum_{t \in leaves} w_tG(t)\\\\
    & where \ w_t = \frac{n(t)}{N}\\
    \end{align*}
    $$
    
        Example:



    
* **Feature Selection:** Choose the **Lowest** $C(T)$
    
    

## References
[wikipedia](https://en.wikipedia.org/wiki/Decision_tree)