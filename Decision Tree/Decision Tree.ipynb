{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several reminders of this code:\n",
    "\n",
    "   1. This is a rewriten code references to tutorial from [Josh Gordon](https://github.com/random-forests), since my original code is not in perfect structure.\n",
    "   2. This code includes ID3 and CART algorithm. \n",
    "   3. Input data should be pandas DataFrame\n",
    "   3. Strongly suggest using hot encoding for categorical data, especially for those categorical features with integers.\n",
    "   4. Feel free to contact me if you think there are some problems.\n",
    "   5. ***Pruning is not included yet, I will work on it very soon.***\n",
    "   6. ***I will add more hyperparameters into this code.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    # This function is to determine if a value is numerical\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "def unique_vals(dataset, feature):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return sorted(set(dataset[feature]))\n",
    "\n",
    "def class_counts(dataset):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        dataset: {DataFrame}, shape = [m_samples, n_features]\n",
    "            Dateset matrix, where 'm_samples' is the number of samples \n",
    "            and 'n_features' is the number of features\n",
    "\n",
    "        Return\n",
    "        ------------------\n",
    "        classCounts: dictionary {class: counts}\n",
    "\n",
    "        \"\"\"\n",
    "        classCounts = collections.defaultdict(int)\n",
    "\n",
    "        # the number of unique elements and their occurrence\n",
    "        for label in dataset.iloc[:,-1]:\n",
    "            if label not in classCounts:\n",
    "                classCounts[label] = 0\n",
    "            classCounts[label] += 1\n",
    "        return classCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split:\n",
    "\n",
    "    def __init__(self, dataset, feature, splitPoint):\n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.splitPoint = splitPoint\n",
    "\n",
    "    def match(self, comparision_sample):\n",
    "        # This is the function that compare the plit point we choosed to a given value\n",
    "        # If it is numerical, then True if splitPoint >= comparision_sample, False if splitPoint < comparision_sample\n",
    "        # If it is categorical, then True if splitPoint == comparision_sample, False if splitPoint != comparision_sample\n",
    "        val = comparision_sample[self.feature]\n",
    "\n",
    "        if is_numeric(val):\n",
    "            return self.splitPoint <= val\n",
    "        else:\n",
    "            return self.splitPoint == val \n",
    "        \n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        header = list(self.dataset.columns)\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.splitPoint):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[header==self.feature], condition, str(self.splitPoint))\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\" A leaf node\n",
    "\n",
    "    A leaf node holds data including unique values and their counts in a dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.prediction = class_counts(dataset)\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"A decision node\n",
    "\n",
    "    A decision node holds the split method and its two child tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, split, left_tree, right_tree):\n",
    "        self.split = split\n",
    "        self.left_tree = left_tree\n",
    "        self.right_tree = right_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self, criterion = 'entropy'):\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            self.info_gain = self._gini_gain\n",
    "        elif self.criterion == 'entropy':\n",
    "            self.info_gain = self._entropy_gain\n",
    "    \n",
    "    \n",
    "    def partition(self, dataset, split):\n",
    "        \"\"\"partition the dataset into left and right\n",
    "\n",
    "        For each value a in a feature, compare it to the split point. Partition the dataset into two subsets.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        left_set = dataset[split.match(dataset)]\n",
    "        \n",
    "        right_set = dataset[split.match(dataset)==False]\n",
    "\n",
    "        return left_set, right_set\n",
    "    \n",
    "    def gini_Impurity(self, dataset):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "\n",
    "        Return\n",
    "        ------------------\n",
    "        giniImpurity: float\n",
    "\n",
    "        \"\"\"\n",
    "        classCounts = class_counts(dataset)\n",
    "        # calculate gini index\n",
    "        gini = 1.0\n",
    "        for key in classCounts:\n",
    "            # calculate occurrence\n",
    "            prob = float(classCounts[key]/(len(dataset)*1.0))\n",
    "            # calculate entropy\n",
    "            gini -= prob ** 2 \n",
    "\n",
    "        return gini\n",
    "    \n",
    "    def _gini_gain(self, dataset, left, right):\n",
    "        \"\"\"Gini gain\n",
    "\n",
    "        \"\"\"\n",
    "        gini = self.gini_Impurity(dataset)\n",
    "        p = float(len(left) / (len(left) +len(right)))\n",
    "        \n",
    "        return gini - float(p * self.gini_Impurity(left)) - float((1-p) * self.gini_Impurity(right)) \n",
    "    \n",
    "    def calcShannonEntropy(self, dataset):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        y_labels: {array-like}, shape = [n_samples, 1]\n",
    "            Dateset matrix, where 'n_samples' is the number of samples \n",
    "            and one column of labels\n",
    "        \n",
    "        Return\n",
    "        ------------------\n",
    "        shannonEntropy: float\n",
    "        \n",
    "        \"\"\"\n",
    "        m, n = dataset.shape\n",
    "        \n",
    "        labelCounts = collections.defaultdict(int)\n",
    "        \n",
    "        classCounts = class_counts(dataset)\n",
    "            \n",
    "        # calculate shannon entropy\n",
    "        shannonEntropy = 0.0\n",
    "        for key in classCounts:\n",
    "            # calculate occurrence\n",
    "            prob = float(classCounts[key]/m)\n",
    "            if prob == 0:\n",
    "                continue\n",
    "            # calculate entropy\n",
    "            shannonEntropy -= prob * np.log2(prob)\n",
    "        \n",
    "        return shannonEntropy\n",
    "    \n",
    "    def _entropy_gain(self, dataset, left, right):\n",
    "        \"\"\"Entropy gain\n",
    "\n",
    "        \"\"\"\n",
    "        entropy = self.calcShannonEntropy(dataset)\n",
    "        p = float(len(left) / (len(left) +len(right)))\n",
    "        \n",
    "        return entropy - float(p*self.calcShannonEntropy(left)) -  float((1-p)*self.calcShannonEntropy(right))\n",
    "   \n",
    "\n",
    "    def find_best_split(self, dataset):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        X_data: {array-like} discrete features\n",
    "        preprocessed dataset, could not deal with continuous features, \n",
    "        and categorical feature should be better as binary form\n",
    "\n",
    "        y_labels: {array-like}\n",
    "\n",
    "        Return\n",
    "        ------------------\n",
    "        best_gain: the maximum gini index gain\n",
    "        best_split: the feature and split point that get the maximum gini index gain\n",
    "\n",
    "        \"\"\"\n",
    "        # Exclude Labels\n",
    "        features = list(dataset.columns[:-1])\n",
    "\n",
    "\n",
    "        best_gain = 0.0\n",
    "        best_split = None\n",
    "\n",
    "        # Loops:\n",
    "        # First for loop: features\n",
    "        # Second for loop: unique values in a feature.\n",
    "\n",
    "        for feature in features:\n",
    "\n",
    "            unique_val = unique_vals(dataset, feature)\n",
    "\n",
    "            for val in unique_val:\n",
    "\n",
    "                split = Split(dataset, feature, val)\n",
    "\n",
    "                left_set, right_set = self.partition(dataset, split)\n",
    "\n",
    "                # Skip this split if it doesn't divide the dataset\n",
    "                if len(left_set) == 0 or len(right_set) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.info_gain(dataset, left_set, right_set)\n",
    "\n",
    "                if gain >= best_gain:\n",
    "                    best_gain = gain \n",
    "                    best_split = split\n",
    "\n",
    "        return best_gain, best_split\n",
    "\n",
    "    \n",
    "    def fit(self,dataset):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #Step1: Find the best split feature and point, and create a root.\n",
    "        gain, split = self.find_best_split(dataset)\n",
    "\n",
    "        # If there is no gain, or gain is less than a threshold\n",
    "        # We will not split any more\n",
    "        # And left it as a leaf\n",
    "        if gain == 0:\n",
    "            return Leaf(dataset)\n",
    "\n",
    "        # Partition the dataset into two sub-trees\n",
    "        left_set, right_set = self.partition(dataset, split)\n",
    "\n",
    "        # Recursively build sub-trees, start from left to right\n",
    "        left_tree = self.fit(left_set)\n",
    "        right_tree = self.fit(right_set)\n",
    "        #\n",
    "        \n",
    "        return Decision_Node(split, left_tree, right_tree)\n",
    "    \n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        \n",
    "        \"\"\"Copy from\n",
    "        World's most elegant tree printing function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, Leaf):\n",
    "            print (spacing + \"Predict\", self.print_leaf(node.prediction))\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.split))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.left_tree, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.right_tree, spacing + \"  \")\n",
    "\n",
    "    def classify(self, dataset, node):\n",
    "        \n",
    "        \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.prediction\n",
    "\n",
    "        # Decide whether to follow the true-branch or the false-branch.\n",
    "        # Compare the feature / value stored in the node,\n",
    "        # to the example we're considering.\n",
    "        if node.split.match(dataset):\n",
    "            return self.classify(dataset, node.left_tree)\n",
    "        else:\n",
    "            return self.classify(dataset, node.right_tree)\n",
    "        \n",
    "    def print_leaf(self, counts):\n",
    "        \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "        total = sum(counts.values()) * 1.0\n",
    "        probs = {}\n",
    "        for lbl in counts.keys():\n",
    "            probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "        return probs\n",
    "    \n",
    "    def save_tree(self, inputTree, filename):\n",
    "        import pickle\n",
    "        fw = open(filename, 'wb')\n",
    "        pickle.dump(inputTree, fw)\n",
    "        fw.close()\n",
    "        \n",
    "    def read_tree(self, filename):\n",
    "        import pickle\n",
    "        tr = open(filename,'rb')\n",
    "        return pickle.load(tr)\n",
    "    \n",
    "    def predict(self, dataset, node):\n",
    "        m,n = dataset.shape\n",
    "        for i in range(m):\n",
    "            leaf = self.classify(dataset.iloc[i],node)\n",
    "            print('The prediction is: {}'.format(self.print_leaf(leaf)))\n",
    "            print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('test.csv')\n",
    "data2 = data2.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dt.fit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.save_tree(tree,'tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dt.read_tree('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is color >= 3?\n",
      "--> True:\n",
      "  Is color == Yellow?\n",
      "  --> True:\n",
      "    Predict {'Apple': '50%', 'Lemon': '50%'}\n",
      "  --> False:\n",
      "    Predict {'Apple': '100%'}\n",
      "--> False:\n",
      "  Predict {'Grape': '100%'}\n"
     ]
    }
   ],
   "source": [
    "dt.print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is: {'Apple': '100%'}\n",
      "\n",
      "\n",
      "The prediction is: {'Apple': '50%', 'Lemon': '50%'}\n",
      "\n",
      "\n",
      "The prediction is: {'Grape': '100%'}\n",
      "\n",
      "\n",
      "The prediction is: {'Grape': '100%'}\n",
      "\n",
      "\n",
      "The prediction is: {'Apple': '50%', 'Lemon': '50%'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.predict(data2,tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
